{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn ##可以插入不同层东西\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset =  pd.read_csv('./dataset/stock_2.csv',delimiter=',',dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_code</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>money</th>\n",
       "      <th>change</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/20</td>\n",
       "      <td>104.3</td>\n",
       "      <td>104.39</td>\n",
       "      <td>99.98</td>\n",
       "      <td>104.39</td>\n",
       "      <td>197000</td>\n",
       "      <td>85000</td>\n",
       "      <td>0.044108822</td>\n",
       "      <td>109.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/21</td>\n",
       "      <td>109.07</td>\n",
       "      <td>109.13</td>\n",
       "      <td>103.73</td>\n",
       "      <td>109.13</td>\n",
       "      <td>28000</td>\n",
       "      <td>16100</td>\n",
       "      <td>0.045406648</td>\n",
       "      <td>114.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/24</td>\n",
       "      <td>113.57</td>\n",
       "      <td>114.55</td>\n",
       "      <td>109.13</td>\n",
       "      <td>114.55</td>\n",
       "      <td>32000</td>\n",
       "      <td>31100</td>\n",
       "      <td>0.049665537</td>\n",
       "      <td>120.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/25</td>\n",
       "      <td>120.09</td>\n",
       "      <td>120.25</td>\n",
       "      <td>114.55</td>\n",
       "      <td>120.25</td>\n",
       "      <td>15000</td>\n",
       "      <td>6500</td>\n",
       "      <td>0.04975993</td>\n",
       "      <td>125.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sh000001</td>\n",
       "      <td>1990/12/26</td>\n",
       "      <td>125.27</td>\n",
       "      <td>125.27</td>\n",
       "      <td>120.25</td>\n",
       "      <td>125.27</td>\n",
       "      <td>100000</td>\n",
       "      <td>53700</td>\n",
       "      <td>0.041746362</td>\n",
       "      <td>125.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index_code        date    open   close     low    high  volume  money  \\\n",
       "0   sh000001  1990/12/20   104.3  104.39   99.98  104.39  197000  85000   \n",
       "1   sh000001  1990/12/21  109.07  109.13  103.73  109.13   28000  16100   \n",
       "2   sh000001  1990/12/24  113.57  114.55  109.13  114.55   32000  31100   \n",
       "3   sh000001  1990/12/25  120.09  120.25  114.55  120.25   15000   6500   \n",
       "4   sh000001  1990/12/26  125.27  125.27  120.25  125.27  100000  53700   \n",
       "\n",
       "        change   label  \n",
       "0  0.044108822  109.13  \n",
       "1  0.045406648  114.55  \n",
       "2  0.049665537  120.25  \n",
       "3   0.04975993  125.27  \n",
       "4  0.041746362  125.28  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二种导入方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_numpy = np.loadtxt('./dataset/stock_2.csv',delimiter=',',dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['index_code', 'date', 'open', 'close', 'low', 'high', 'volume',\n",
       "        'money', 'change', 'label'],\n",
       "       ['sh000001', '1990/12/20', '104.3', '104.39', '99.98', '104.39',\n",
       "        '197000', '85000', '0.044108822', '109.13'],\n",
       "       ['sh000001', '1990/12/21', '109.07', '109.13', '103.73', '109.13',\n",
       "        '28000', '16100', '0.045406648', '114.55'],\n",
       "       ['sh000001', '1990/12/24', '113.57', '114.55', '109.13', '114.55',\n",
       "        '32000', '31100', '0.049665537', '120.25'],\n",
       "       ['sh000001', '1990/12/25', '120.09', '120.25', '114.55', '120.25',\n",
       "        '15000', '6500', '0.04975993', '125.27']], dtype='<U12')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_numpy[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用pandas 对导入数据进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6109 entries, 0 to 6108\n",
      "Data columns (total 10 columns):\n",
      "index_code    6109 non-null object\n",
      "date          6109 non-null object\n",
      "open          6109 non-null object\n",
      "close         6109 non-null object\n",
      "low           6109 non-null object\n",
      "high          6109 non-null object\n",
      "volume        6109 non-null object\n",
      "money         6109 non-null object\n",
      "change        6109 non-null object\n",
      "label         6109 non-null object\n",
      "dtypes: object(10)\n",
      "memory usage: 477.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## panda 数据跑 numpy 矩阵 网络只能导入矩阵，不可以是pandas矩阵数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_np = dataset.values #什么意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sh000001', '1990/12/20', '104.3', '104.39', '99.98', '104.39',\n",
       "        '197000', '85000', '0.044108822', '109.13'],\n",
       "       ['sh000001', '1990/12/21', '109.07', '109.13', '103.73', '109.13',\n",
       "        '28000', '16100', '0.045406648', '114.55'],\n",
       "       ['sh000001', '1990/12/24', '113.57', '114.55', '109.13', '114.55',\n",
       "        '32000', '31100', '0.049665537', '120.25'],\n",
       "       ['sh000001', '1990/12/25', '120.09', '120.25', '114.55', '120.25',\n",
       "        '15000', '6500', '0.04975993', '125.27'],\n",
       "       ['sh000001', '1990/12/26', '125.27', '125.27', '120.25', '125.27',\n",
       "        '100000', '53700', '0.041746362', '125.28']], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_np[:5,:] #查看前面五行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去掉前面连个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6109, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_np.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_array = dataset_np[:,2:] #前闭后开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6109, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['104.3', '104.39', '99.98', '104.39', '197000', '85000',\n",
       "        '0.044108822', '109.13'],\n",
       "       ['109.07', '109.13', '103.73', '109.13', '28000', '16100',\n",
       "        '0.045406648', '114.55'],\n",
       "       ['113.57', '114.55', '109.13', '114.55', '32000', '31100',\n",
       "        '0.049665537', '120.25'],\n",
       "       ['120.09', '120.25', '114.55', '120.25', '15000', '6500',\n",
       "        '0.04975993', '125.27'],\n",
       "       ['125.27', '125.27', '120.25', '125.27', '100000', '53700',\n",
       "        '0.041746362', '125.28']], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_array[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将object 类型强转为float 32类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_array =dataset_array.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6109, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0430000e+02, 1.0439000e+02, 9.9980003e+01, 1.0439000e+02,\n",
       "        1.9700000e+05, 8.5000000e+04, 4.4108823e-02, 1.0913000e+02],\n",
       "       [1.0907000e+02, 1.0913000e+02, 1.0373000e+02, 1.0913000e+02,\n",
       "        2.8000000e+04, 1.6100000e+04, 4.5406647e-02, 1.1455000e+02],\n",
       "       [1.1357000e+02, 1.1455000e+02, 1.0913000e+02, 1.1455000e+02,\n",
       "        3.2000000e+04, 3.1100000e+04, 4.9665537e-02, 1.2025000e+02],\n",
       "       [1.2009000e+02, 1.2025000e+02, 1.1455000e+02, 1.2025000e+02,\n",
       "        1.5000000e+04, 6.5000000e+03, 4.9759932e-02, 1.2527000e+02],\n",
       "       [1.2527000e+02, 1.2527000e+02, 1.2025000e+02, 1.2527000e+02,\n",
       "        1.0000000e+05, 5.3700000e+04, 4.1746363e-02, 1.2528000e+02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_array[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分割数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =dataset_array[:,:-1]\n",
    "y =dataset_array[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6109, 7)\n",
      "(6109, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标注化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard = StandardScaler()\n",
    "standard.fit(X_train)\n",
    "\n",
    "X_train_standard = standard.transform(X_train)\n",
    "X_test_standard = standard.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4887, 7)\n",
      "(1222, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_standard.shape)\n",
    "print(X_test_standard.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4887, 1)\n",
      "(1222, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型 全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): ## 固定形式不需要问\n",
    "    def __init__ (self):\n",
    "        super(Net,self).__init__()\n",
    "        ## input shape:[-1,7]\n",
    "        self.hidden1 = nn.Linear(7,64)## 64是代表神经元个数超参数，要测试不同的神经元个数\n",
    "        self.hidden2 = nn.Linear(64,32)\n",
    "        self.hidden3 = nn.Linear(32,10)\n",
    "        \n",
    "        ## output layer\n",
    "        self.pred = nn.Linear(10,1) #[-1,10]*[10,1]=[-1,1] \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))## 非线性变化\n",
    "        out = self.pred(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden1): Linear(in_features=7, out_features=64, bias=True)\n",
       "  (hidden2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (hidden3): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (pred): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.01)##随机梯度下降\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 4314218.0\n",
      "Epoch: 2 Loss: 4313651.5\n",
      "Epoch: 3 Loss: 4313321.0\n",
      "Epoch: 4 Loss: 4312856.0\n",
      "Epoch: 5 Loss: 4312188.5\n",
      "Epoch: 6 Loss: 4311289.5\n",
      "Epoch: 7 Loss: 4310101.5\n",
      "Epoch: 8 Loss: 4308550.5\n",
      "Epoch: 9 Loss: 4306539.5\n",
      "Epoch: 10 Loss: 4303948.0\n",
      "Epoch: 11 Loss: 4300647.0\n",
      "Epoch: 12 Loss: 4296487.0\n",
      "Epoch: 13 Loss: 4291296.5\n",
      "Epoch: 14 Loss: 4284891.0\n",
      "Epoch: 15 Loss: 4277049.5\n",
      "Epoch: 16 Loss: 4267518.5\n",
      "Epoch: 17 Loss: 4256015.5\n",
      "Epoch: 18 Loss: 4242230.0\n",
      "Epoch: 19 Loss: 4225808.0\n",
      "Epoch: 20 Loss: 4206357.5\n",
      "Epoch: 21 Loss: 4183451.2\n",
      "Epoch: 22 Loss: 4156620.2\n",
      "Epoch: 23 Loss: 4125344.2\n",
      "Epoch: 24 Loss: 4089053.5\n",
      "Epoch: 25 Loss: 4047062.2\n",
      "Epoch: 26 Loss: 3998590.2\n",
      "Epoch: 27 Loss: 3943055.5\n",
      "Epoch: 28 Loss: 3879762.2\n",
      "Epoch: 29 Loss: 3808009.8\n",
      "Epoch: 30 Loss: 3727092.2\n",
      "Epoch: 31 Loss: 3636320.2\n",
      "Epoch: 32 Loss: 3535058.0\n",
      "Epoch: 33 Loss: 3422742.5\n",
      "Epoch: 34 Loss: 3298931.2\n",
      "Epoch: 35 Loss: 3163339.2\n",
      "Epoch: 36 Loss: 3015903.8\n",
      "Epoch: 37 Loss: 2856851.0\n",
      "Epoch: 38 Loss: 2686774.8\n",
      "Epoch: 39 Loss: 2506731.0\n",
      "Epoch: 40 Loss: 2318347.8\n",
      "Epoch: 41 Loss: 2123948.2\n",
      "Epoch: 42 Loss: 1926680.9\n",
      "Epoch: 43 Loss: 1730647.9\n",
      "Epoch: 44 Loss: 1541013.5\n",
      "Epoch: 45 Loss: 1364039.0\n",
      "Epoch: 46 Loss: 1206983.6\n",
      "Epoch: 47 Loss: 1077716.6\n",
      "Epoch: 48 Loss: 983801.75\n",
      "Epoch: 49 Loss: 930778.7\n",
      "Epoch: 50 Loss: 919608.5\n",
      "Epoch: 51 Loss: 944021.44\n",
      "Epoch: 52 Loss: 989656.6\n",
      "Epoch: 53 Loss: 1037022.2\n",
      "Epoch: 54 Loss: 1067941.4\n",
      "Epoch: 55 Loss: 1071692.8\n",
      "Epoch: 56 Loss: 1047042.3\n",
      "Epoch: 57 Loss: 1000060.06\n",
      "Epoch: 58 Loss: 940296.94\n",
      "Epoch: 59 Loss: 877453.75\n",
      "Epoch: 60 Loss: 819276.2\n",
      "Epoch: 61 Loss: 770650.1\n",
      "Epoch: 62 Loss: 733602.6\n",
      "Epoch: 63 Loss: 707831.44\n",
      "Epoch: 64 Loss: 691445.5\n",
      "Epoch: 65 Loss: 681718.4\n",
      "Epoch: 66 Loss: 675762.0\n",
      "Epoch: 67 Loss: 670958.0\n",
      "Epoch: 68 Loss: 665225.9\n",
      "Epoch: 69 Loss: 657121.8\n",
      "Epoch: 70 Loss: 645843.94\n",
      "Epoch: 71 Loss: 631141.25\n",
      "Epoch: 72 Loss: 613241.9\n",
      "Epoch: 73 Loss: 592727.0\n",
      "Epoch: 74 Loss: 570406.4\n",
      "Epoch: 75 Loss: 547211.75\n",
      "Epoch: 76 Loss: 524061.6\n",
      "Epoch: 77 Loss: 501789.6\n",
      "Epoch: 78 Loss: 481027.4\n",
      "Epoch: 79 Loss: 462153.34\n",
      "Epoch: 80 Loss: 445242.3\n",
      "Epoch: 81 Loss: 430080.66\n",
      "Epoch: 82 Loss: 416227.5\n",
      "Epoch: 83 Loss: 403112.8\n",
      "Epoch: 84 Loss: 390143.03\n",
      "Epoch: 85 Loss: 376811.4\n",
      "Epoch: 86 Loss: 362798.75\n",
      "Epoch: 87 Loss: 347999.22\n",
      "Epoch: 88 Loss: 332515.4\n",
      "Epoch: 89 Loss: 316615.7\n",
      "Epoch: 90 Loss: 300644.12\n",
      "Epoch: 91 Loss: 284957.66\n",
      "Epoch: 92 Loss: 269855.1\n",
      "Epoch: 93 Loss: 255537.95\n",
      "Epoch: 94 Loss: 242084.2\n",
      "Epoch: 95 Loss: 229469.64\n",
      "Epoch: 96 Loss: 217627.28\n",
      "Epoch: 97 Loss: 206432.69\n",
      "Epoch: 98 Loss: 195761.7\n",
      "Epoch: 99 Loss: 185513.17\n",
      "Epoch: 100 Loss: 175622.06\n",
      "Epoch: 101 Loss: 166074.64\n",
      "Epoch: 102 Loss: 156884.2\n",
      "Epoch: 103 Loss: 148108.31\n",
      "Epoch: 104 Loss: 139831.27\n",
      "Epoch: 105 Loss: 132128.08\n",
      "Epoch: 106 Loss: 125044.94\n",
      "Epoch: 107 Loss: 118617.43\n",
      "Epoch: 108 Loss: 112864.07\n",
      "Epoch: 109 Loss: 107734.3\n",
      "Epoch: 110 Loss: 103149.086\n",
      "Epoch: 111 Loss: 99012.13\n",
      "Epoch: 112 Loss: 95244.664\n",
      "Epoch: 113 Loss: 91774.445\n",
      "Epoch: 114 Loss: 88540.625\n",
      "Epoch: 115 Loss: 85500.53\n",
      "Epoch: 116 Loss: 82629.99\n",
      "Epoch: 117 Loss: 79923.52\n",
      "Epoch: 118 Loss: 77383.266\n",
      "Epoch: 119 Loss: 75017.61\n",
      "Epoch: 120 Loss: 72826.6\n",
      "Epoch: 121 Loss: 70819.38\n",
      "Epoch: 122 Loss: 68969.58\n",
      "Epoch: 123 Loss: 67244.766\n",
      "Epoch: 124 Loss: 65614.13\n",
      "Epoch: 125 Loss: 64060.59\n",
      "Epoch: 126 Loss: 62542.812\n",
      "Epoch: 127 Loss: 61048.38\n",
      "Epoch: 128 Loss: 59564.8\n",
      "Epoch: 129 Loss: 58088.87\n",
      "Epoch: 130 Loss: 56620.76\n",
      "Epoch: 131 Loss: 55167.66\n",
      "Epoch: 132 Loss: 53753.35\n",
      "Epoch: 133 Loss: 52383.062\n",
      "Epoch: 134 Loss: 51042.965\n",
      "Epoch: 135 Loss: 49733.555\n",
      "Epoch: 136 Loss: 48454.035\n",
      "Epoch: 137 Loss: 47203.715\n",
      "Epoch: 138 Loss: 45977.49\n",
      "Epoch: 139 Loss: 44778.176\n",
      "Epoch: 140 Loss: 43607.53\n",
      "Epoch: 141 Loss: 42467.363\n",
      "Epoch: 142 Loss: 41355.84\n",
      "Epoch: 143 Loss: 40271.668\n",
      "Epoch: 144 Loss: 39222.746\n",
      "Epoch: 145 Loss: 38216.836\n",
      "Epoch: 146 Loss: 37245.58\n",
      "Epoch: 147 Loss: 36311.445\n",
      "Epoch: 148 Loss: 35417.47\n",
      "Epoch: 149 Loss: 34561.87\n",
      "Epoch: 150 Loss: 33738.105\n",
      "Epoch: 151 Loss: 32942.54\n",
      "Epoch: 152 Loss: 32172.33\n",
      "Epoch: 153 Loss: 31429.346\n",
      "Epoch: 154 Loss: 30712.873\n",
      "Epoch: 155 Loss: 30017.516\n",
      "Epoch: 156 Loss: 29343.51\n",
      "Epoch: 157 Loss: 28691.152\n",
      "Epoch: 158 Loss: 28060.87\n",
      "Epoch: 159 Loss: 27448.045\n",
      "Epoch: 160 Loss: 26855.36\n",
      "Epoch: 161 Loss: 26279.572\n",
      "Epoch: 162 Loss: 25720.371\n",
      "Epoch: 163 Loss: 25172.883\n",
      "Epoch: 164 Loss: 24636.604\n",
      "Epoch: 165 Loss: 24111.09\n",
      "Epoch: 166 Loss: 23594.11\n",
      "Epoch: 167 Loss: 23086.582\n",
      "Epoch: 168 Loss: 22588.19\n",
      "Epoch: 169 Loss: 22099.17\n",
      "Epoch: 170 Loss: 21619.666\n",
      "Epoch: 171 Loss: 21149.162\n",
      "Epoch: 172 Loss: 20687.55\n",
      "Epoch: 173 Loss: 20235.035\n",
      "Epoch: 174 Loss: 19791.555\n",
      "Epoch: 175 Loss: 19355.9\n",
      "Epoch: 176 Loss: 18927.951\n",
      "Epoch: 177 Loss: 18507.592\n",
      "Epoch: 178 Loss: 18096.268\n",
      "Epoch: 179 Loss: 17693.613\n",
      "Epoch: 180 Loss: 17300.45\n",
      "Epoch: 181 Loss: 16914.799\n",
      "Epoch: 182 Loss: 16536.344\n",
      "Epoch: 183 Loss: 16165.478\n",
      "Epoch: 184 Loss: 15802.403\n",
      "Epoch: 185 Loss: 15447.283\n",
      "Epoch: 186 Loss: 15099.578\n",
      "Epoch: 187 Loss: 14758.826\n",
      "Epoch: 188 Loss: 14425.215\n",
      "Epoch: 189 Loss: 14097.961\n",
      "Epoch: 190 Loss: 13776.802\n",
      "Epoch: 191 Loss: 13462.6\n",
      "Epoch: 192 Loss: 13155.073\n",
      "Epoch: 193 Loss: 12854.501\n",
      "Epoch: 194 Loss: 12561.588\n",
      "Epoch: 195 Loss: 12275.811\n",
      "Epoch: 196 Loss: 11997.723\n",
      "Epoch: 197 Loss: 11729.103\n",
      "Epoch: 198 Loss: 11470.228\n",
      "Epoch: 199 Loss: 11218.37\n",
      "Epoch: 200 Loss: 10974.207\n",
      "Epoch: 201 Loss: 10736.65\n",
      "Epoch: 202 Loss: 10507.2\n",
      "Epoch: 203 Loss: 10284.956\n",
      "Epoch: 204 Loss: 10070.016\n",
      "Epoch: 205 Loss: 9861.34\n",
      "Epoch: 206 Loss: 9658.425\n",
      "Epoch: 207 Loss: 9460.884\n",
      "Epoch: 208 Loss: 9268.544\n",
      "Epoch: 209 Loss: 9081.249\n",
      "Epoch: 210 Loss: 8898.596\n",
      "Epoch: 211 Loss: 8720.689\n",
      "Epoch: 212 Loss: 8547.184\n",
      "Epoch: 213 Loss: 8377.687\n",
      "Epoch: 214 Loss: 8212.215\n",
      "Epoch: 215 Loss: 8050.7124\n",
      "Epoch: 216 Loss: 7892.7373\n",
      "Epoch: 217 Loss: 7738.427\n",
      "Epoch: 218 Loss: 7587.3804\n",
      "Epoch: 219 Loss: 7439.909\n",
      "Epoch: 220 Loss: 7296.052\n",
      "Epoch: 221 Loss: 7155.2207\n",
      "Epoch: 222 Loss: 7017.3384\n",
      "Epoch: 223 Loss: 6882.8887\n",
      "Epoch: 224 Loss: 6752.5835\n",
      "Epoch: 225 Loss: 6625.588\n",
      "Epoch: 226 Loss: 6501.7354\n",
      "Epoch: 227 Loss: 6381.1465\n",
      "Epoch: 228 Loss: 6263.8965\n",
      "Epoch: 229 Loss: 6150.0874\n",
      "Epoch: 230 Loss: 6039.2856\n",
      "Epoch: 231 Loss: 5931.392\n",
      "Epoch: 232 Loss: 5826.358\n",
      "Epoch: 233 Loss: 5724.334\n",
      "Epoch: 234 Loss: 5625.3857\n",
      "Epoch: 235 Loss: 5530.062\n",
      "Epoch: 236 Loss: 5438.388\n",
      "Epoch: 237 Loss: 5349.885\n",
      "Epoch: 238 Loss: 5264.0293\n",
      "Epoch: 239 Loss: 5180.6445\n",
      "Epoch: 240 Loss: 5099.921\n",
      "Epoch: 241 Loss: 5021.492\n",
      "Epoch: 242 Loss: 4945.19\n",
      "Epoch: 243 Loss: 4871.005\n",
      "Epoch: 244 Loss: 4798.654\n",
      "Epoch: 245 Loss: 4727.896\n",
      "Epoch: 246 Loss: 4658.7974\n",
      "Epoch: 247 Loss: 4591.5205\n",
      "Epoch: 248 Loss: 4525.9727\n",
      "Epoch: 249 Loss: 4461.9194\n",
      "Epoch: 250 Loss: 4399.3257\n",
      "Epoch: 251 Loss: 4338.358\n",
      "Epoch: 252 Loss: 4279.1064\n",
      "Epoch: 253 Loss: 4221.5425\n",
      "Epoch: 254 Loss: 4165.2754\n",
      "Epoch: 255 Loss: 4110.494\n",
      "Epoch: 256 Loss: 4057.1765\n",
      "Epoch: 257 Loss: 4005.5164\n",
      "Epoch: 258 Loss: 3955.8604\n",
      "Epoch: 259 Loss: 3907.9685\n",
      "Epoch: 260 Loss: 3862.095\n",
      "Epoch: 261 Loss: 3817.6248\n",
      "Epoch: 262 Loss: 3774.1006\n",
      "Epoch: 263 Loss: 3731.3042\n",
      "Epoch: 264 Loss: 3689.3496\n",
      "Epoch: 265 Loss: 3648.2346\n",
      "Epoch: 266 Loss: 3607.8813\n",
      "Epoch: 267 Loss: 3568.2385\n",
      "Epoch: 268 Loss: 3529.2346\n",
      "Epoch: 269 Loss: 3490.8994\n",
      "Epoch: 270 Loss: 3453.2544\n",
      "Epoch: 271 Loss: 3416.2332\n",
      "Epoch: 272 Loss: 3379.8452\n",
      "Epoch: 273 Loss: 3344.1904\n",
      "Epoch: 274 Loss: 3309.1523\n",
      "Epoch: 275 Loss: 3274.7773\n",
      "Epoch: 276 Loss: 3241.0078\n",
      "Epoch: 277 Loss: 3207.7656\n",
      "Epoch: 278 Loss: 3175.1003\n",
      "Epoch: 279 Loss: 3142.9773\n",
      "Epoch: 280 Loss: 3111.346\n",
      "Epoch: 281 Loss: 3080.331\n",
      "Epoch: 282 Loss: 3049.873\n",
      "Epoch: 283 Loss: 3019.7874\n",
      "Epoch: 284 Loss: 2990.1348\n",
      "Epoch: 285 Loss: 2960.9768\n",
      "Epoch: 286 Loss: 2932.3684\n",
      "Epoch: 287 Loss: 2904.1848\n",
      "Epoch: 288 Loss: 2876.451\n",
      "Epoch: 289 Loss: 2849.1367\n",
      "Epoch: 290 Loss: 2822.4065\n",
      "Epoch: 291 Loss: 2796.1284\n",
      "Epoch: 292 Loss: 2770.279\n",
      "Epoch: 293 Loss: 2744.9678\n",
      "Epoch: 294 Loss: 2720.399\n",
      "Epoch: 295 Loss: 2696.5867\n",
      "Epoch: 296 Loss: 2673.3823\n",
      "Epoch: 297 Loss: 2650.8198\n",
      "Epoch: 298 Loss: 2628.696\n",
      "Epoch: 299 Loss: 2606.8743\n",
      "Epoch: 300 Loss: 2585.3496\n",
      "Epoch: 301 Loss: 2564.1736\n",
      "Epoch: 302 Loss: 2543.3418\n",
      "Epoch: 303 Loss: 2522.8406\n",
      "Epoch: 304 Loss: 2502.7598\n",
      "Epoch: 305 Loss: 2483.0032\n",
      "Epoch: 306 Loss: 2463.4727\n",
      "Epoch: 307 Loss: 2444.2412\n",
      "Epoch: 308 Loss: 2425.2998\n",
      "Epoch: 309 Loss: 2406.638\n",
      "Epoch: 310 Loss: 2388.1785\n",
      "Epoch: 311 Loss: 2370.0264\n",
      "Epoch: 312 Loss: 2352.1277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 313 Loss: 2334.447\n",
      "Epoch: 314 Loss: 2317.0217\n",
      "Epoch: 315 Loss: 2299.8044\n",
      "Epoch: 316 Loss: 2282.872\n",
      "Epoch: 317 Loss: 2266.2036\n",
      "Epoch: 318 Loss: 2249.7495\n",
      "Epoch: 319 Loss: 2233.5254\n",
      "Epoch: 320 Loss: 2217.555\n",
      "Epoch: 321 Loss: 2201.8516\n",
      "Epoch: 322 Loss: 2186.321\n",
      "Epoch: 323 Loss: 2170.965\n",
      "Epoch: 324 Loss: 2155.7788\n",
      "Epoch: 325 Loss: 2140.8145\n",
      "Epoch: 326 Loss: 2126.0557\n",
      "Epoch: 327 Loss: 2111.487\n",
      "Epoch: 328 Loss: 2097.0676\n",
      "Epoch: 329 Loss: 2082.8354\n",
      "Epoch: 330 Loss: 2068.8806\n",
      "Epoch: 331 Loss: 2055.2156\n",
      "Epoch: 332 Loss: 2041.7725\n",
      "Epoch: 333 Loss: 2028.4532\n",
      "Epoch: 334 Loss: 2015.2677\n",
      "Epoch: 335 Loss: 2002.2572\n",
      "Epoch: 336 Loss: 1989.4332\n",
      "Epoch: 337 Loss: 1976.8073\n",
      "Epoch: 338 Loss: 1964.3649\n",
      "Epoch: 339 Loss: 1952.0763\n",
      "Epoch: 340 Loss: 1939.9552\n",
      "Epoch: 341 Loss: 1927.9861\n",
      "Epoch: 342 Loss: 1916.1425\n",
      "Epoch: 343 Loss: 1904.4845\n",
      "Epoch: 344 Loss: 1892.973\n",
      "Epoch: 345 Loss: 1881.7144\n",
      "Epoch: 346 Loss: 1870.5912\n",
      "Epoch: 347 Loss: 1859.6599\n",
      "Epoch: 348 Loss: 1848.8351\n",
      "Epoch: 349 Loss: 1838.157\n",
      "Epoch: 350 Loss: 1827.6862\n",
      "Epoch: 351 Loss: 1817.364\n",
      "Epoch: 352 Loss: 1807.1567\n",
      "Epoch: 353 Loss: 1797.0472\n",
      "Epoch: 354 Loss: 1787.0819\n",
      "Epoch: 355 Loss: 1777.2457\n",
      "Epoch: 356 Loss: 1767.5522\n",
      "Epoch: 357 Loss: 1758.0225\n",
      "Epoch: 358 Loss: 1748.6798\n",
      "Epoch: 359 Loss: 1739.4453\n",
      "Epoch: 360 Loss: 1730.3207\n",
      "Epoch: 361 Loss: 1721.3196\n",
      "Epoch: 362 Loss: 1712.4827\n",
      "Epoch: 363 Loss: 1703.7567\n",
      "Epoch: 364 Loss: 1695.1978\n",
      "Epoch: 365 Loss: 1686.7583\n",
      "Epoch: 366 Loss: 1678.4458\n",
      "Epoch: 367 Loss: 1670.23\n",
      "Epoch: 368 Loss: 1662.0726\n",
      "Epoch: 369 Loss: 1654.0457\n",
      "Epoch: 370 Loss: 1646.1166\n",
      "Epoch: 371 Loss: 1638.2765\n",
      "Epoch: 372 Loss: 1630.5187\n",
      "Epoch: 373 Loss: 1622.8501\n",
      "Epoch: 374 Loss: 1615.2517\n",
      "Epoch: 375 Loss: 1607.6757\n",
      "Epoch: 376 Loss: 1600.2253\n",
      "Epoch: 377 Loss: 1592.8293\n",
      "Epoch: 378 Loss: 1585.5243\n",
      "Epoch: 379 Loss: 1578.2648\n",
      "Epoch: 380 Loss: 1571.0673\n",
      "Epoch: 381 Loss: 1563.989\n",
      "Epoch: 382 Loss: 1557.0006\n",
      "Epoch: 383 Loss: 1550.0695\n",
      "Epoch: 384 Loss: 1543.1854\n",
      "Epoch: 385 Loss: 1536.3854\n",
      "Epoch: 386 Loss: 1529.6166\n",
      "Epoch: 387 Loss: 1522.9423\n",
      "Epoch: 388 Loss: 1516.3306\n",
      "Epoch: 389 Loss: 1509.8042\n",
      "Epoch: 390 Loss: 1503.255\n",
      "Epoch: 391 Loss: 1496.7439\n",
      "Epoch: 392 Loss: 1490.2898\n",
      "Epoch: 393 Loss: 1483.9012\n",
      "Epoch: 394 Loss: 1477.5684\n",
      "Epoch: 395 Loss: 1471.2983\n",
      "Epoch: 396 Loss: 1465.0917\n",
      "Epoch: 397 Loss: 1458.9286\n",
      "Epoch: 398 Loss: 1452.8209\n",
      "Epoch: 399 Loss: 1446.7928\n",
      "Epoch: 400 Loss: 1440.8575\n",
      "Epoch: 401 Loss: 1434.9965\n",
      "Epoch: 402 Loss: 1429.1925\n",
      "Epoch: 403 Loss: 1423.4839\n",
      "Epoch: 404 Loss: 1417.841\n",
      "Epoch: 405 Loss: 1412.3423\n",
      "Epoch: 406 Loss: 1406.925\n",
      "Epoch: 407 Loss: 1401.5819\n",
      "Epoch: 408 Loss: 1396.3381\n",
      "Epoch: 409 Loss: 1391.1263\n",
      "Epoch: 410 Loss: 1385.9584\n",
      "Epoch: 411 Loss: 1380.9484\n",
      "Epoch: 412 Loss: 1376.0205\n",
      "Epoch: 413 Loss: 1371.156\n",
      "Epoch: 414 Loss: 1366.3552\n",
      "Epoch: 415 Loss: 1361.611\n",
      "Epoch: 416 Loss: 1356.9149\n",
      "Epoch: 417 Loss: 1352.2816\n",
      "Epoch: 418 Loss: 1347.7102\n",
      "Epoch: 419 Loss: 1343.2356\n",
      "Epoch: 420 Loss: 1338.7775\n",
      "Epoch: 421 Loss: 1334.3796\n",
      "Epoch: 422 Loss: 1330.033\n",
      "Epoch: 423 Loss: 1325.7327\n",
      "Epoch: 424 Loss: 1321.5382\n",
      "Epoch: 425 Loss: 1317.412\n",
      "Epoch: 426 Loss: 1313.3342\n",
      "Epoch: 427 Loss: 1309.3173\n",
      "Epoch: 428 Loss: 1305.3273\n",
      "Epoch: 429 Loss: 1301.4487\n",
      "Epoch: 430 Loss: 1297.6309\n",
      "Epoch: 431 Loss: 1293.9094\n",
      "Epoch: 432 Loss: 1290.27\n",
      "Epoch: 433 Loss: 1286.6678\n",
      "Epoch: 434 Loss: 1283.1481\n",
      "Epoch: 435 Loss: 1279.7051\n",
      "Epoch: 436 Loss: 1276.3265\n",
      "Epoch: 437 Loss: 1273.0072\n",
      "Epoch: 438 Loss: 1269.7218\n",
      "Epoch: 439 Loss: 1266.4896\n",
      "Epoch: 440 Loss: 1263.289\n",
      "Epoch: 441 Loss: 1260.1364\n",
      "Epoch: 442 Loss: 1257.0219\n",
      "Epoch: 443 Loss: 1253.9594\n",
      "Epoch: 444 Loss: 1250.9642\n",
      "Epoch: 445 Loss: 1248.025\n",
      "Epoch: 446 Loss: 1245.1195\n",
      "Epoch: 447 Loss: 1242.2666\n",
      "Epoch: 448 Loss: 1239.4465\n",
      "Epoch: 449 Loss: 1236.655\n",
      "Epoch: 450 Loss: 1233.9032\n",
      "Epoch: 451 Loss: 1231.1964\n",
      "Epoch: 452 Loss: 1228.5143\n",
      "Epoch: 453 Loss: 1225.8911\n",
      "Epoch: 454 Loss: 1223.2906\n",
      "Epoch: 455 Loss: 1220.737\n",
      "Epoch: 456 Loss: 1218.2192\n",
      "Epoch: 457 Loss: 1215.7412\n",
      "Epoch: 458 Loss: 1213.3037\n",
      "Epoch: 459 Loss: 1210.8884\n",
      "Epoch: 460 Loss: 1208.5154\n",
      "Epoch: 461 Loss: 1206.181\n",
      "Epoch: 462 Loss: 1203.8761\n",
      "Epoch: 463 Loss: 1201.5901\n",
      "Epoch: 464 Loss: 1199.3342\n",
      "Epoch: 465 Loss: 1197.1195\n",
      "Epoch: 466 Loss: 1194.933\n",
      "Epoch: 467 Loss: 1192.7765\n",
      "Epoch: 468 Loss: 1190.6481\n",
      "Epoch: 469 Loss: 1188.5522\n",
      "Epoch: 470 Loss: 1186.4775\n",
      "Epoch: 471 Loss: 1184.4043\n",
      "Epoch: 472 Loss: 1182.3757\n",
      "Epoch: 473 Loss: 1180.3606\n",
      "Epoch: 474 Loss: 1178.3613\n",
      "Epoch: 475 Loss: 1176.3794\n",
      "Epoch: 476 Loss: 1174.389\n",
      "Epoch: 477 Loss: 1172.4229\n",
      "Epoch: 478 Loss: 1170.4688\n",
      "Epoch: 479 Loss: 1168.549\n",
      "Epoch: 480 Loss: 1166.6504\n",
      "Epoch: 481 Loss: 1164.7474\n",
      "Epoch: 482 Loss: 1162.8627\n",
      "Epoch: 483 Loss: 1161.0074\n",
      "Epoch: 484 Loss: 1159.1641\n",
      "Epoch: 485 Loss: 1157.3213\n",
      "Epoch: 486 Loss: 1155.5066\n",
      "Epoch: 487 Loss: 1153.7096\n",
      "Epoch: 488 Loss: 1151.9072\n",
      "Epoch: 489 Loss: 1150.1064\n",
      "Epoch: 490 Loss: 1148.3301\n",
      "Epoch: 491 Loss: 1146.5609\n",
      "Epoch: 492 Loss: 1144.812\n",
      "Epoch: 493 Loss: 1143.0953\n",
      "Epoch: 494 Loss: 1141.3969\n",
      "Epoch: 495 Loss: 1139.6959\n",
      "Epoch: 496 Loss: 1138.0123\n",
      "Epoch: 497 Loss: 1136.3484\n",
      "Epoch: 498 Loss: 1134.6901\n",
      "Epoch: 499 Loss: 1133.0552\n",
      "Epoch: 500 Loss: 1131.4346\n",
      "Epoch: 501 Loss: 1129.8138\n",
      "Epoch: 502 Loss: 1128.2019\n",
      "Epoch: 503 Loss: 1126.6105\n",
      "Epoch: 504 Loss: 1125.0422\n",
      "Epoch: 505 Loss: 1123.4904\n",
      "Epoch: 506 Loss: 1121.9667\n",
      "Epoch: 507 Loss: 1120.4623\n",
      "Epoch: 508 Loss: 1118.9706\n",
      "Epoch: 509 Loss: 1117.4775\n",
      "Epoch: 510 Loss: 1116.0131\n",
      "Epoch: 511 Loss: 1114.5603\n",
      "Epoch: 512 Loss: 1113.1301\n",
      "Epoch: 513 Loss: 1111.7067\n",
      "Epoch: 514 Loss: 1110.2897\n",
      "Epoch: 515 Loss: 1108.8954\n",
      "Epoch: 516 Loss: 1107.5173\n",
      "Epoch: 517 Loss: 1106.1698\n",
      "Epoch: 518 Loss: 1104.8271\n",
      "Epoch: 519 Loss: 1103.4733\n",
      "Epoch: 520 Loss: 1102.1527\n",
      "Epoch: 521 Loss: 1100.8413\n",
      "Epoch: 522 Loss: 1099.5345\n",
      "Epoch: 523 Loss: 1098.2269\n",
      "Epoch: 524 Loss: 1096.9429\n",
      "Epoch: 525 Loss: 1095.6615\n",
      "Epoch: 526 Loss: 1094.3671\n",
      "Epoch: 527 Loss: 1093.1415\n",
      "Epoch: 528 Loss: 1091.9204\n",
      "Epoch: 529 Loss: 1090.6904\n",
      "Epoch: 530 Loss: 1089.4471\n",
      "Epoch: 531 Loss: 1088.2158\n",
      "Epoch: 532 Loss: 1087.0103\n",
      "Epoch: 533 Loss: 1085.7936\n",
      "Epoch: 534 Loss: 1084.5986\n",
      "Epoch: 535 Loss: 1083.4048\n",
      "Epoch: 536 Loss: 1082.2312\n",
      "Epoch: 537 Loss: 1081.0414\n",
      "Epoch: 538 Loss: 1079.8328\n",
      "Epoch: 539 Loss: 1078.6365\n",
      "Epoch: 540 Loss: 1077.4717\n",
      "Epoch: 541 Loss: 1076.3057\n",
      "Epoch: 542 Loss: 1075.1388\n",
      "Epoch: 543 Loss: 1073.9716\n",
      "Epoch: 544 Loss: 1072.817\n",
      "Epoch: 545 Loss: 1071.6786\n",
      "Epoch: 546 Loss: 1070.5422\n",
      "Epoch: 547 Loss: 1069.4072\n",
      "Epoch: 548 Loss: 1068.2812\n",
      "Epoch: 549 Loss: 1067.1537\n",
      "Epoch: 550 Loss: 1066.0109\n",
      "Epoch: 551 Loss: 1064.8793\n",
      "Epoch: 552 Loss: 1063.7526\n",
      "Epoch: 553 Loss: 1062.6373\n",
      "Epoch: 554 Loss: 1061.5012\n",
      "Epoch: 555 Loss: 1060.3824\n",
      "Epoch: 556 Loss: 1059.2826\n",
      "Epoch: 557 Loss: 1058.1769\n",
      "Epoch: 558 Loss: 1057.0662\n",
      "Epoch: 559 Loss: 1055.9764\n",
      "Epoch: 560 Loss: 1054.8883\n",
      "Epoch: 561 Loss: 1053.7845\n",
      "Epoch: 562 Loss: 1052.6665\n",
      "Epoch: 563 Loss: 1051.5651\n",
      "Epoch: 564 Loss: 1050.4453\n",
      "Epoch: 565 Loss: 1049.3226\n",
      "Epoch: 566 Loss: 1048.2096\n",
      "Epoch: 567 Loss: 1047.1072\n",
      "Epoch: 568 Loss: 1046.0062\n",
      "Epoch: 569 Loss: 1044.9231\n",
      "Epoch: 570 Loss: 1043.8453\n",
      "Epoch: 571 Loss: 1042.7545\n",
      "Epoch: 572 Loss: 1041.6887\n",
      "Epoch: 573 Loss: 1040.6042\n",
      "Epoch: 574 Loss: 1039.5258\n",
      "Epoch: 575 Loss: 1038.4563\n",
      "Epoch: 576 Loss: 1037.3906\n",
      "Epoch: 577 Loss: 1036.3406\n",
      "Epoch: 578 Loss: 1035.2825\n",
      "Epoch: 579 Loss: 1034.2255\n",
      "Epoch: 580 Loss: 1033.1934\n",
      "Epoch: 581 Loss: 1032.151\n",
      "Epoch: 582 Loss: 1031.1243\n",
      "Epoch: 583 Loss: 1030.1287\n",
      "Epoch: 584 Loss: 1029.114\n",
      "Epoch: 585 Loss: 1028.1332\n",
      "Epoch: 586 Loss: 1027.1595\n",
      "Epoch: 587 Loss: 1026.1814\n",
      "Epoch: 588 Loss: 1025.212\n",
      "Epoch: 589 Loss: 1024.2557\n",
      "Epoch: 590 Loss: 1023.2894\n",
      "Epoch: 591 Loss: 1022.3534\n",
      "Epoch: 592 Loss: 1021.44135\n",
      "Epoch: 593 Loss: 1020.5339\n",
      "Epoch: 594 Loss: 1019.61194\n",
      "Epoch: 595 Loss: 1018.7174\n",
      "Epoch: 596 Loss: 1017.83813\n",
      "Epoch: 597 Loss: 1016.9604\n",
      "Epoch: 598 Loss: 1016.0896\n",
      "Epoch: 599 Loss: 1015.21136\n",
      "Epoch: 600 Loss: 1014.3489\n",
      "Epoch: 601 Loss: 1013.4951\n",
      "Epoch: 602 Loss: 1012.6368\n",
      "Epoch: 603 Loss: 1011.7918\n",
      "Epoch: 604 Loss: 1010.9793\n",
      "Epoch: 605 Loss: 1010.16473\n",
      "Epoch: 606 Loss: 1009.3316\n",
      "Epoch: 607 Loss: 1008.50195\n",
      "Epoch: 608 Loss: 1007.7009\n",
      "Epoch: 609 Loss: 1006.8796\n",
      "Epoch: 610 Loss: 1006.05115\n",
      "Epoch: 611 Loss: 1005.2245\n",
      "Epoch: 612 Loss: 1004.416\n",
      "Epoch: 613 Loss: 1003.62695\n",
      "Epoch: 614 Loss: 1002.8292\n",
      "Epoch: 615 Loss: 1002.0171\n",
      "Epoch: 616 Loss: 1001.20514\n",
      "Epoch: 617 Loss: 1000.4126\n",
      "Epoch: 618 Loss: 999.63416\n",
      "Epoch: 619 Loss: 998.8446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 620 Loss: 998.061\n",
      "Epoch: 621 Loss: 997.2711\n",
      "Epoch: 622 Loss: 996.5005\n",
      "Epoch: 623 Loss: 995.7427\n",
      "Epoch: 624 Loss: 994.9998\n",
      "Epoch: 625 Loss: 994.28455\n",
      "Epoch: 626 Loss: 993.5917\n",
      "Epoch: 627 Loss: 992.8986\n",
      "Epoch: 628 Loss: 992.218\n",
      "Epoch: 629 Loss: 991.5138\n",
      "Epoch: 630 Loss: 990.82117\n",
      "Epoch: 631 Loss: 990.13434\n",
      "Epoch: 632 Loss: 989.4507\n",
      "Epoch: 633 Loss: 988.75916\n",
      "Epoch: 634 Loss: 988.07886\n",
      "Epoch: 635 Loss: 987.3979\n",
      "Epoch: 636 Loss: 986.72705\n",
      "Epoch: 637 Loss: 986.07\n",
      "Epoch: 638 Loss: 985.44556\n",
      "Epoch: 639 Loss: 984.79926\n",
      "Epoch: 640 Loss: 984.13025\n",
      "Epoch: 641 Loss: 983.4962\n",
      "Epoch: 642 Loss: 982.8581\n",
      "Epoch: 643 Loss: 982.2354\n",
      "Epoch: 644 Loss: 981.6032\n",
      "Epoch: 645 Loss: 980.97675\n",
      "Epoch: 646 Loss: 980.3425\n",
      "Epoch: 647 Loss: 979.7218\n",
      "Epoch: 648 Loss: 979.11554\n",
      "Epoch: 649 Loss: 978.5089\n",
      "Epoch: 650 Loss: 977.9113\n",
      "Epoch: 651 Loss: 977.31195\n",
      "Epoch: 652 Loss: 976.72424\n",
      "Epoch: 653 Loss: 976.1329\n",
      "Epoch: 654 Loss: 975.53937\n",
      "Epoch: 655 Loss: 974.9782\n",
      "Epoch: 656 Loss: 974.4025\n",
      "Epoch: 657 Loss: 973.8107\n",
      "Epoch: 658 Loss: 973.2576\n",
      "Epoch: 659 Loss: 972.6982\n",
      "Epoch: 660 Loss: 972.1313\n",
      "Epoch: 661 Loss: 971.58264\n",
      "Epoch: 662 Loss: 971.0387\n",
      "Epoch: 663 Loss: 970.4908\n",
      "Epoch: 664 Loss: 969.9581\n",
      "Epoch: 665 Loss: 969.42847\n",
      "Epoch: 666 Loss: 968.90137\n",
      "Epoch: 667 Loss: 968.3812\n",
      "Epoch: 668 Loss: 967.8678\n",
      "Epoch: 669 Loss: 967.3614\n",
      "Epoch: 670 Loss: 966.85565\n",
      "Epoch: 671 Loss: 966.36414\n",
      "Epoch: 672 Loss: 965.85565\n",
      "Epoch: 673 Loss: 965.3548\n",
      "Epoch: 674 Loss: 964.8572\n",
      "Epoch: 675 Loss: 964.3705\n",
      "Epoch: 676 Loss: 963.88336\n",
      "Epoch: 677 Loss: 963.3859\n",
      "Epoch: 678 Loss: 962.9065\n",
      "Epoch: 679 Loss: 962.4292\n",
      "Epoch: 680 Loss: 961.9604\n",
      "Epoch: 681 Loss: 961.4838\n",
      "Epoch: 682 Loss: 961.01404\n",
      "Epoch: 683 Loss: 960.5485\n",
      "Epoch: 684 Loss: 960.071\n",
      "Epoch: 685 Loss: 959.5997\n",
      "Epoch: 686 Loss: 959.1496\n",
      "Epoch: 687 Loss: 958.6898\n",
      "Epoch: 688 Loss: 958.2068\n",
      "Epoch: 689 Loss: 957.72046\n",
      "Epoch: 690 Loss: 957.27203\n",
      "Epoch: 691 Loss: 956.8156\n",
      "Epoch: 692 Loss: 956.3448\n",
      "Epoch: 693 Loss: 955.88684\n",
      "Epoch: 694 Loss: 955.4171\n",
      "Epoch: 695 Loss: 954.95557\n",
      "Epoch: 696 Loss: 954.4862\n",
      "Epoch: 697 Loss: 954.01434\n",
      "Epoch: 698 Loss: 953.5552\n",
      "Epoch: 699 Loss: 953.0982\n",
      "Epoch: 700 Loss: 952.6421\n",
      "Epoch: 701 Loss: 952.1853\n",
      "Epoch: 702 Loss: 951.7218\n",
      "Epoch: 703 Loss: 951.2701\n",
      "Epoch: 704 Loss: 950.81213\n",
      "Epoch: 705 Loss: 950.3495\n",
      "Epoch: 706 Loss: 949.90344\n",
      "Epoch: 707 Loss: 949.45264\n",
      "Epoch: 708 Loss: 949.00226\n",
      "Epoch: 709 Loss: 948.5483\n",
      "Epoch: 710 Loss: 948.1017\n",
      "Epoch: 711 Loss: 947.655\n",
      "Epoch: 712 Loss: 947.2029\n",
      "Epoch: 713 Loss: 946.75055\n",
      "Epoch: 714 Loss: 946.29865\n",
      "Epoch: 715 Loss: 945.8466\n",
      "Epoch: 716 Loss: 945.3926\n",
      "Epoch: 717 Loss: 944.94977\n",
      "Epoch: 718 Loss: 944.5082\n",
      "Epoch: 719 Loss: 944.0622\n",
      "Epoch: 720 Loss: 943.6327\n",
      "Epoch: 721 Loss: 943.1983\n",
      "Epoch: 722 Loss: 942.782\n",
      "Epoch: 723 Loss: 942.3478\n",
      "Epoch: 724 Loss: 941.9239\n",
      "Epoch: 725 Loss: 941.49786\n",
      "Epoch: 726 Loss: 941.0678\n",
      "Epoch: 727 Loss: 940.64087\n",
      "Epoch: 728 Loss: 940.2344\n",
      "Epoch: 729 Loss: 939.8094\n",
      "Epoch: 730 Loss: 939.4036\n",
      "Epoch: 731 Loss: 938.9891\n",
      "Epoch: 732 Loss: 938.5746\n",
      "Epoch: 733 Loss: 938.16473\n",
      "Epoch: 734 Loss: 937.75946\n",
      "Epoch: 735 Loss: 937.36017\n",
      "Epoch: 736 Loss: 936.9667\n",
      "Epoch: 737 Loss: 936.55994\n",
      "Epoch: 738 Loss: 936.16327\n",
      "Epoch: 739 Loss: 935.76746\n",
      "Epoch: 740 Loss: 935.3725\n",
      "Epoch: 741 Loss: 934.9818\n",
      "Epoch: 742 Loss: 934.5882\n",
      "Epoch: 743 Loss: 934.205\n",
      "Epoch: 744 Loss: 933.82\n",
      "Epoch: 745 Loss: 933.4294\n",
      "Epoch: 746 Loss: 933.036\n",
      "Epoch: 747 Loss: 932.6587\n",
      "Epoch: 748 Loss: 932.2875\n",
      "Epoch: 749 Loss: 931.903\n",
      "Epoch: 750 Loss: 931.52795\n",
      "Epoch: 751 Loss: 931.1572\n",
      "Epoch: 752 Loss: 930.77155\n",
      "Epoch: 753 Loss: 930.4024\n",
      "Epoch: 754 Loss: 930.02783\n",
      "Epoch: 755 Loss: 929.6539\n",
      "Epoch: 756 Loss: 929.2912\n",
      "Epoch: 757 Loss: 928.92\n",
      "Epoch: 758 Loss: 928.5479\n",
      "Epoch: 759 Loss: 928.17804\n",
      "Epoch: 760 Loss: 927.81494\n",
      "Epoch: 761 Loss: 927.4455\n",
      "Epoch: 762 Loss: 927.0933\n",
      "Epoch: 763 Loss: 926.72266\n",
      "Epoch: 764 Loss: 926.3437\n",
      "Epoch: 765 Loss: 925.9828\n",
      "Epoch: 766 Loss: 925.626\n",
      "Epoch: 767 Loss: 925.277\n",
      "Epoch: 768 Loss: 924.9249\n",
      "Epoch: 769 Loss: 924.57275\n",
      "Epoch: 770 Loss: 924.2252\n",
      "Epoch: 771 Loss: 923.87537\n",
      "Epoch: 772 Loss: 923.5301\n",
      "Epoch: 773 Loss: 923.198\n",
      "Epoch: 774 Loss: 922.8551\n",
      "Epoch: 775 Loss: 922.5318\n",
      "Epoch: 776 Loss: 922.20966\n",
      "Epoch: 777 Loss: 921.88354\n",
      "Epoch: 778 Loss: 921.57007\n",
      "Epoch: 779 Loss: 921.25745\n",
      "Epoch: 780 Loss: 920.94965\n",
      "Epoch: 781 Loss: 920.64417\n",
      "Epoch: 782 Loss: 920.3498\n",
      "Epoch: 783 Loss: 919.9812\n",
      "Epoch: 784 Loss: 920.17523\n",
      "Epoch: 785 Loss: 919.43506\n",
      "Epoch: 786 Loss: 919.1572\n",
      "Epoch: 787 Loss: 918.921\n",
      "Epoch: 788 Loss: 918.6133\n",
      "Epoch: 789 Loss: 918.2604\n",
      "Epoch: 790 Loss: 917.87805\n",
      "Epoch: 791 Loss: 917.44794\n",
      "Epoch: 792 Loss: 916.9351\n",
      "Epoch: 793 Loss: 916.28186\n",
      "Epoch: 794 Loss: 915.4607\n",
      "Epoch: 795 Loss: 914.45435\n",
      "Epoch: 796 Loss: 913.26056\n",
      "Epoch: 797 Loss: 911.52997\n",
      "Epoch: 798 Loss: 911.53796\n",
      "Epoch: 799 Loss: 908.1761\n",
      "Epoch: 800 Loss: 907.38837\n",
      "Epoch: 801 Loss: 904.8422\n",
      "Epoch: 802 Loss: 901.86035\n",
      "Epoch: 803 Loss: 900.7469\n",
      "Epoch: 804 Loss: 897.2386\n",
      "Epoch: 805 Loss: 895.0308\n",
      "Epoch: 806 Loss: 892.7056\n",
      "Epoch: 807 Loss: 889.4292\n",
      "Epoch: 808 Loss: 886.9249\n",
      "Epoch: 809 Loss: 884.36444\n",
      "Epoch: 810 Loss: 881.38214\n",
      "Epoch: 811 Loss: 879.2729\n",
      "Epoch: 812 Loss: 876.3988\n",
      "Epoch: 813 Loss: 874.58673\n",
      "Epoch: 814 Loss: 871.88306\n",
      "Epoch: 815 Loss: 870.0126\n",
      "Epoch: 816 Loss: 868.085\n",
      "Epoch: 817 Loss: 866.46\n",
      "Epoch: 818 Loss: 864.9891\n",
      "Epoch: 819 Loss: 863.43115\n",
      "Epoch: 820 Loss: 862.1821\n",
      "Epoch: 821 Loss: 861.05054\n",
      "Epoch: 822 Loss: 859.92316\n",
      "Epoch: 823 Loss: 858.93024\n",
      "Epoch: 824 Loss: 858.16394\n",
      "Epoch: 825 Loss: 857.53094\n",
      "Epoch: 826 Loss: 856.9622\n",
      "Epoch: 827 Loss: 856.8642\n",
      "Epoch: 828 Loss: 856.2525\n",
      "Epoch: 829 Loss: 856.0425\n",
      "Epoch: 830 Loss: 855.61414\n",
      "Epoch: 831 Loss: 855.6229\n",
      "Epoch: 832 Loss: 855.3345\n",
      "Epoch: 833 Loss: 855.31805\n",
      "Epoch: 834 Loss: 855.0339\n",
      "Epoch: 835 Loss: 854.83545\n",
      "Epoch: 836 Loss: 854.68195\n",
      "Epoch: 837 Loss: 854.36597\n",
      "Epoch: 838 Loss: 854.24097\n",
      "Epoch: 839 Loss: 853.97565\n",
      "Epoch: 840 Loss: 853.6942\n",
      "Epoch: 841 Loss: 853.52075\n",
      "Epoch: 842 Loss: 853.16943\n",
      "Epoch: 843 Loss: 852.99005\n",
      "Epoch: 844 Loss: 852.66754\n",
      "Epoch: 845 Loss: 852.4356\n",
      "Epoch: 846 Loss: 852.22974\n",
      "Epoch: 847 Loss: 851.9249\n",
      "Epoch: 848 Loss: 851.76025\n",
      "Epoch: 849 Loss: 851.48315\n",
      "Epoch: 850 Loss: 851.2774\n",
      "Epoch: 851 Loss: 851.0728\n",
      "Epoch: 852 Loss: 850.8155\n",
      "Epoch: 853 Loss: 850.6529\n",
      "Epoch: 854 Loss: 850.40625\n",
      "Epoch: 855 Loss: 850.2375\n",
      "Epoch: 856 Loss: 850.036\n",
      "Epoch: 857 Loss: 849.84094\n",
      "Epoch: 858 Loss: 849.68195\n",
      "Epoch: 859 Loss: 849.47986\n",
      "Epoch: 860 Loss: 849.32904\n",
      "Epoch: 861 Loss: 849.14374\n",
      "Epoch: 862 Loss: 848.9762\n",
      "Epoch: 863 Loss: 848.81146\n",
      "Epoch: 864 Loss: 848.63165\n",
      "Epoch: 865 Loss: 848.4774\n",
      "Epoch: 866 Loss: 848.29645\n",
      "Epoch: 867 Loss: 848.1395\n",
      "Epoch: 868 Loss: 847.968\n",
      "Epoch: 869 Loss: 847.8077\n",
      "Epoch: 870 Loss: 847.63824\n",
      "Epoch: 871 Loss: 847.4771\n",
      "Epoch: 872 Loss: 847.3078\n",
      "Epoch: 873 Loss: 847.1513\n",
      "Epoch: 874 Loss: 846.9855\n",
      "Epoch: 875 Loss: 846.82526\n",
      "Epoch: 876 Loss: 846.66113\n",
      "Epoch: 877 Loss: 846.50903\n",
      "Epoch: 878 Loss: 846.34875\n",
      "Epoch: 879 Loss: 846.1931\n",
      "Epoch: 880 Loss: 846.03656\n",
      "Epoch: 881 Loss: 845.88477\n",
      "Epoch: 882 Loss: 845.7353\n",
      "Epoch: 883 Loss: 845.5839\n",
      "Epoch: 884 Loss: 845.43726\n",
      "Epoch: 885 Loss: 845.28864\n",
      "Epoch: 886 Loss: 845.146\n",
      "Epoch: 887 Loss: 844.9973\n",
      "Epoch: 888 Loss: 844.86066\n",
      "Epoch: 889 Loss: 844.70776\n",
      "Epoch: 890 Loss: 844.56903\n",
      "Epoch: 891 Loss: 844.42194\n",
      "Epoch: 892 Loss: 844.28314\n",
      "Epoch: 893 Loss: 844.1434\n",
      "Epoch: 894 Loss: 844.0049\n",
      "Epoch: 895 Loss: 843.86774\n",
      "Epoch: 896 Loss: 843.7327\n",
      "Epoch: 897 Loss: 843.59607\n",
      "Epoch: 898 Loss: 843.4603\n",
      "Epoch: 899 Loss: 843.32654\n",
      "Epoch: 900 Loss: 843.19385\n",
      "Epoch: 901 Loss: 843.0599\n",
      "Epoch: 902 Loss: 842.92865\n",
      "Epoch: 903 Loss: 842.7957\n",
      "Epoch: 904 Loss: 842.66504\n",
      "Epoch: 905 Loss: 842.5345\n",
      "Epoch: 906 Loss: 842.4048\n",
      "Epoch: 907 Loss: 842.27594\n",
      "Epoch: 908 Loss: 842.1476\n",
      "Epoch: 909 Loss: 842.01965\n",
      "Epoch: 910 Loss: 841.8926\n",
      "Epoch: 911 Loss: 841.7666\n",
      "Epoch: 912 Loss: 841.6404\n",
      "Epoch: 913 Loss: 841.51556\n",
      "Epoch: 914 Loss: 841.39075\n",
      "Epoch: 915 Loss: 841.26697\n",
      "Epoch: 916 Loss: 841.1433\n",
      "Epoch: 917 Loss: 841.0204\n",
      "Epoch: 918 Loss: 840.8983\n",
      "Epoch: 919 Loss: 840.7761\n",
      "Epoch: 920 Loss: 840.6551\n",
      "Epoch: 921 Loss: 840.53485\n",
      "Epoch: 922 Loss: 840.4149\n",
      "Epoch: 923 Loss: 840.29535\n",
      "Epoch: 924 Loss: 840.1762\n",
      "Epoch: 925 Loss: 840.0585\n",
      "Epoch: 926 Loss: 839.9416\n",
      "Epoch: 927 Loss: 839.8236\n",
      "Epoch: 928 Loss: 839.7067\n",
      "Epoch: 929 Loss: 839.5909\n",
      "Epoch: 930 Loss: 839.47565\n",
      "Epoch: 931 Loss: 839.3604\n",
      "Epoch: 932 Loss: 839.2464\n",
      "Epoch: 933 Loss: 839.1321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 934 Loss: 839.0185\n",
      "Epoch: 935 Loss: 838.9053\n",
      "Epoch: 936 Loss: 838.7922\n",
      "Epoch: 937 Loss: 838.6798\n",
      "Epoch: 938 Loss: 838.5676\n",
      "Epoch: 939 Loss: 838.4566\n",
      "Epoch: 940 Loss: 838.3449\n",
      "Epoch: 941 Loss: 838.234\n",
      "Epoch: 942 Loss: 838.12445\n",
      "Epoch: 943 Loss: 838.0172\n",
      "Epoch: 944 Loss: 837.90466\n",
      "Epoch: 945 Loss: 837.7865\n",
      "Epoch: 946 Loss: 837.67804\n",
      "Epoch: 947 Loss: 837.55646\n",
      "Epoch: 948 Loss: 837.4472\n",
      "Epoch: 949 Loss: 837.32697\n",
      "Epoch: 950 Loss: 837.22754\n",
      "Epoch: 951 Loss: 837.10693\n",
      "Epoch: 952 Loss: 837.0054\n",
      "Epoch: 953 Loss: 836.88794\n",
      "Epoch: 954 Loss: 836.7802\n",
      "Epoch: 955 Loss: 836.6658\n",
      "Epoch: 956 Loss: 836.55554\n",
      "Epoch: 957 Loss: 836.44714\n",
      "Epoch: 958 Loss: 836.34\n",
      "Epoch: 959 Loss: 836.2326\n",
      "Epoch: 960 Loss: 836.12714\n",
      "Epoch: 961 Loss: 836.0204\n",
      "Epoch: 962 Loss: 835.91364\n",
      "Epoch: 963 Loss: 835.81714\n",
      "Epoch: 964 Loss: 835.69794\n",
      "Epoch: 965 Loss: 835.59973\n",
      "Epoch: 966 Loss: 835.5039\n",
      "Epoch: 967 Loss: 835.3931\n",
      "Epoch: 968 Loss: 835.30096\n",
      "Epoch: 969 Loss: 835.19147\n",
      "Epoch: 970 Loss: 835.0904\n",
      "Epoch: 971 Loss: 834.9865\n",
      "Epoch: 972 Loss: 834.8759\n",
      "Epoch: 973 Loss: 834.7741\n",
      "Epoch: 974 Loss: 834.67926\n",
      "Epoch: 975 Loss: 834.56976\n",
      "Epoch: 976 Loss: 834.46967\n",
      "Epoch: 977 Loss: 834.3673\n",
      "Epoch: 978 Loss: 834.2654\n",
      "Epoch: 979 Loss: 834.16595\n",
      "Epoch: 980 Loss: 834.0684\n",
      "Epoch: 981 Loss: 833.9625\n",
      "Epoch: 982 Loss: 833.8582\n",
      "Epoch: 983 Loss: 833.76575\n",
      "Epoch: 984 Loss: 833.6648\n",
      "Epoch: 985 Loss: 833.55493\n",
      "Epoch: 986 Loss: 833.4722\n",
      "Epoch: 987 Loss: 833.3554\n",
      "Epoch: 988 Loss: 833.2525\n",
      "Epoch: 989 Loss: 833.16644\n",
      "Epoch: 990 Loss: 833.04944\n",
      "Epoch: 991 Loss: 832.9623\n",
      "Epoch: 992 Loss: 832.85913\n",
      "Epoch: 993 Loss: 832.74835\n",
      "Epoch: 994 Loss: 832.6566\n",
      "Epoch: 995 Loss: 832.54755\n",
      "Epoch: 996 Loss: 832.4499\n",
      "Epoch: 997 Loss: 832.3498\n",
      "Epoch: 998 Loss: 832.24475\n",
      "Epoch: 999 Loss: 832.1462\n",
      "Epoch: 1000 Loss: 832.04486\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1000\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # 将array to Tensor（zhan liang）\n",
    "    input_x = Variable(torch.FloatTensor(X_train_standard)) ## 自动求导\n",
    "    input_y = Variable(torch.FloatTensor(y_train))\n",
    "    \n",
    "    y_predict = net(input_x)# 把输入数据 扔进 神经网络 去 预测 出一个值\n",
    "    \n",
    "    loss =loss_func(input_y,y_predict) # 计算 MSE\n",
    "    \n",
    "    optimizer.zero_grad() # 梯度归0  不保存原来的内容\n",
    "    loss.backward() ## 反向传播 梯度下降\n",
    "    optimizer.step() #找到新的梯度\n",
    "    \n",
    "    print('Epoch:',(epoch+1),'Loss:',loss.data.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
